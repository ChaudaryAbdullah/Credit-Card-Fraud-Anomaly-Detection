{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e172f66",
   "metadata": {},
   "source": [
    "# Transformerâ€“GANâ€“Contrastive Anomaly Detection on Credit Card Fraud Dataset\n",
    "\n",
    "End-to-end implementation of a **Transformer-based autoencoder** with **contrastive learning** and a **latent-space WGAN** for anomaly detection on the Kaggle *Credit Card Fraud Detection* dataset.\n",
    "\n",
    "Place `creditcard.csv` at `/mnt/data/creditcard.csv` (or change `DATA_PATH`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a03b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# IMPORTS & SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    roc_curve,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"âœ“ Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "DATA_PATH = \"/mnt/data/creditcard.csv\"  # change if needed\n",
    "\n",
    "# Sliding window\n",
    "WINDOW_SIZE = 30\n",
    "STEP = 1\n",
    "\n",
    "# Model\n",
    "D_MODEL = 128\n",
    "NHEAD = 8\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "LATENT_DIM = 16\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "# Loss weights\n",
    "W_RECON = 1.0\n",
    "W_CONTRAST = 0.05\n",
    "W_ADV = 0.02\n",
    "CONTRAST_WARMUP_EPOCHS = 5\n",
    "\n",
    "# WGAN\n",
    "N_CRITIC = 3\n",
    "CLIP_VALUE = 0.05\n",
    "\n",
    "# Augmentation\n",
    "TIME_MASK_PROB = 0.1\n",
    "FEATURE_MASK_PROB = 0.1\n",
    "NOISE_STD = 0.01\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c9a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING, CLEANING, SEQUENCE GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ðŸ“¥ Loading dataset...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Raw shape:\", df.shape)\n",
    "\n",
    "# Ensure columns\n",
    "assert \"Class\" in df.columns, \"Dataset must contain 'Class' column.\"\n",
    "assert \"Time\" in df.columns, \"Dataset must contain 'Time' column.\"\n",
    "\n",
    "# Clean\n",
    "df = df.dropna(subset=[\"Class\"]).reset_index(drop=True)\n",
    "df[\"Class\"] = df[\"Class\"].astype(float).astype(int)\n",
    "df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "print(\"Cleaned shape:\", df.shape)\n",
    "print(\"Class distribution:\")\n",
    "print(df[\"Class\"].value_counts())\n",
    "\n",
    "# Sort by time\n",
    "df = df.sort_values(by=\"Time\").reset_index(drop=True)\n",
    "print(\"âœ“ Sorted by Time\")\n",
    "\n",
    "# Features & labels\n",
    "features = df.drop(columns=[\"Class\"]).columns.tolist()\n",
    "X_raw = df[features].values\n",
    "y_raw = df[\"Class\"].values\n",
    "\n",
    "print(\"Feature shape:\", X_raw.shape)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)\n",
    "print(\"âœ“ Features standardized\")\n",
    "\n",
    "# Sequence generation\n",
    "def create_sequences(data, labels, window, step):\n",
    "    seqs, seq_labels = [], []\n",
    "    n = len(data)\n",
    "    for i in range(0, n - window, step):\n",
    "        w_data = data[i:i+window]\n",
    "        w_lab = labels[i:i+window]\n",
    "        if np.isnan(w_lab).any():\n",
    "            continue\n",
    "        seqs.append(w_data)\n",
    "        seq_labels.append(int(w_lab.max()))\n",
    "    return np.array(seqs), np.array(seq_labels)\n",
    "\n",
    "print(f\"ðŸ›  Generating sequences (window={WINDOW_SIZE}, step={STEP})...\")\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_raw, WINDOW_SIZE, STEP)\n",
    "\n",
    "print(\"Sequences shape:\", X_seq.shape)\n",
    "print(\"Labels shape:\", y_seq.shape)\n",
    "print(f\"Sequence anomaly ratio: {y_seq.mean()*100:.4f}%\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq,\n",
    "    y_seq,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y_seq\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n",
    "\n",
    "# Tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "print(\"âœ“ Converted to torch tensors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260aab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# DATASET & GEOMETRIC MASKING\n",
    "# ============================================================================\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels=None):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sequences.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.sequences[idx]\n",
    "        if self.labels is None:\n",
    "            return x\n",
    "        y = self.labels[idx]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def geometric_masking(batch, time_mask_prob=0.1, feature_mask_prob=0.1, noise_std=0.01):\n",
    "    augmented = batch.clone()\n",
    "    B, T, F = augmented.shape\n",
    "\n",
    "    if time_mask_prob > 0:\n",
    "        time_mask = (torch.rand(B, T, device=augmented.device) < time_mask_prob).unsqueeze(-1)\n",
    "        augmented = augmented.masked_fill(time_mask, 0.0)\n",
    "\n",
    "    if feature_mask_prob > 0:\n",
    "        feat_mask = (torch.rand(B, 1, F, device=augmented.device) < feature_mask_prob)\n",
    "        augmented = augmented.masked_fill(feat_mask, 0.0)\n",
    "\n",
    "    if noise_std > 0:\n",
    "        augmented = augmented + torch.randn_like(augmented) * noise_std\n",
    "\n",
    "    return augmented\n",
    "\n",
    "print(\"âœ“ SequenceDataset and geometric_masking ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b67319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# POSITIONAL ENCODING\n",
    "# ============================================================================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:, :T]\n",
    "\n",
    "print(\"âœ“ PositionalEncoding defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a276f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# TRANSFORMER AUTOENCODER\n",
    "# ============================================================================\n",
    "\n",
    "class TransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, feat_dim, d_model=64, nhead=4, num_layers=2, latent_dim=16):\n",
    "        super().__init__()\n",
    "        self.input_fc = nn.Linear(feat_dim, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len=500)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=256,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder_tf = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "        self.latent_fc = nn.Linear(d_model, latent_dim)\n",
    "\n",
    "        self.dec_input_fc = nn.Linear(latent_dim, d_model)\n",
    "        dec_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=256,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder_tf = nn.TransformerEncoder(dec_layer, num_layers=2)\n",
    "        self.pos_dec = PositionalEncoding(d_model, max_len=500)\n",
    "        self.output_fc = nn.Linear(d_model, feat_dim)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.input_fc(x)\n",
    "        h = self.pos_enc(h)\n",
    "        h = self.encoder_tf(h)\n",
    "        z = self.latent_fc(h[:, 0])\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, seq_len):\n",
    "        h = self.dec_input_fc(z).unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        h = self.pos_dec(h)\n",
    "        h = self.decoder_tf(h)\n",
    "        out = self.output_fc(h)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        recon = self.decode(z, x.size(1))\n",
    "        return z, recon\n",
    "\n",
    "print(\"âœ“ TransformerAutoencoder defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c5dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# GENERATOR & DISCRIMINATOR (LATENT WGAN)\n",
    "# ============================================================================\n",
    "\n",
    "def build_generator(latent_dim):\n",
    "    gen = nn.Sequential(\n",
    "        nn.Linear(latent_dim, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, latent_dim)\n",
    "    )\n",
    "    for p in gen.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return gen\n",
    "\n",
    "\n",
    "def build_discriminator(latent_dim):\n",
    "    disc = nn.Sequential(\n",
    "        nn.Linear(latent_dim, 128),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Linear(32, 1)\n",
    "    )\n",
    "    for p in disc.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return disc\n",
    "\n",
    "print(\"âœ“ Generator & Discriminator defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e5db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# NT-XENT CONTRASTIVE LOSS\n",
    "# ============================================================================\n",
    "\n",
    "def nt_xent_loss(z1, z2, temperature=0.5):\n",
    "    z1 = F.normalize(z1, dim=1, eps=1e-8)\n",
    "    z2 = F.normalize(z2, dim=1, eps=1e-8)\n",
    "    B = z1.size(0)\n",
    "    z = torch.cat([z1, z2], dim=0)\n",
    "    sim = torch.matmul(z, z.T) / temperature\n",
    "    mask = torch.eye(2*B, dtype=torch.bool, device=z.device)\n",
    "    sim = sim.masked_fill(mask, -1e9)\n",
    "    labels = torch.cat([\n",
    "        torch.arange(B, 2*B),\n",
    "        torch.arange(0, B)\n",
    "    ], dim=0).to(z.device)\n",
    "    loss = F.cross_entropy(sim, labels)\n",
    "    return loss\n",
    "\n",
    "print(\"âœ“ NT-Xent defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d230f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# INSTANTIATE MODELS, OPTIMIZERS, DATALOADER\n",
    "# ============================================================================\n",
    "\n",
    "feat_dim = len(features)\n",
    "\n",
    "model = TransformerAutoencoder(\n",
    "    feat_dim=feat_dim,\n",
    "    d_model=D_MODEL,\n",
    "    nhead=NHEAD,\n",
    "    num_layers=NUM_ENCODER_LAYERS,\n",
    "    latent_dim=LATENT_DIM\n",
    ").to(DEVICE)\n",
    "\n",
    "generator = build_generator(LATENT_DIM).to(DEVICE)\n",
    "discriminator = build_discriminator(LATENT_DIM).to(DEVICE)\n",
    "\n",
    "def count_parameters(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Autoencoder params:\", count_parameters(model))\n",
    "print(\"Generator params:  \", count_parameters(generator))\n",
    "print(\"Discriminator params:\", count_parameters(discriminator))\n",
    "\n",
    "optimizer_ae = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=1e-5,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "optimizer_gen = torch.optim.Adam(\n",
    "    generator.parameters(),\n",
    "    lr=LEARNING_RATE * 0.5,\n",
    "    betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_disc = torch.optim.RMSprop(\n",
    "    discriminator.parameters(),\n",
    "    lr=LEARNING_RATE * 0.25\n",
    ")\n",
    "\n",
    "train_dataset = SequenceDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "train_anom = float(y_train_t.float().mean()) * 100\n",
    "print(f\"âœ“ DataLoader: {len(train_loader)} batches, anomaly ratio {train_anom:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "history = {\n",
    "    \"epoch\": [],\n",
    "    \"recon_loss\": [],\n",
    "    \"contrast_loss\": [],\n",
    "    \"enc_adv_loss\": [],\n",
    "    \"disc_loss\": [],\n",
    "    \"gen_loss\": []\n",
    "}\n",
    "\n",
    "print(\"STARTING TRAINING\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    epoch_recon = 0.0\n",
    "    epoch_contrast = 0.0\n",
    "    epoch_encadv = 0.0\n",
    "    epoch_disc = 0.0\n",
    "    epoch_gen = 0.0\n",
    "\n",
    "    for batch_idx, (seq, labels) in enumerate(train_loader):\n",
    "        seq = seq.to(DEVICE)\n",
    "\n",
    "        aug = geometric_masking(\n",
    "            seq,\n",
    "            time_mask_prob=TIME_MASK_PROB,\n",
    "            feature_mask_prob=FEATURE_MASK_PROB,\n",
    "            noise_std=NOISE_STD\n",
    "        )\n",
    "\n",
    "        # 1) AE + contrastive + encoder adv\n",
    "        optimizer_ae.zero_grad()\n",
    "        z1, recon1 = model(seq)\n",
    "        z2, recon2 = model(aug)\n",
    "\n",
    "        recon_loss = F.mse_loss(recon1, seq)\n",
    "        contrast_loss = nt_xent_loss(z1, z2, temperature=0.5)\n",
    "        contrast_weight = 0.0 if epoch < CONTRAST_WARMUP_EPOCHS else W_CONTRAST\n",
    "        enc_adv = -torch.mean(discriminator(z1))\n",
    "\n",
    "        ae_loss = W_RECON * recon_loss + contrast_weight * contrast_loss + W_ADV * enc_adv\n",
    "        ae_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer_ae.step()\n",
    "\n",
    "        # 2) Discriminator\n",
    "        if batch_idx % N_CRITIC == 0:\n",
    "            for _ in range(N_CRITIC):\n",
    "                optimizer_disc.zero_grad()\n",
    "                noise = torch.randn(seq.size(0), LATENT_DIM, device=DEVICE)\n",
    "                z_fake = generator(noise)\n",
    "                with torch.no_grad():\n",
    "                    z_real = model.encode(seq)\n",
    "                real_scores = discriminator(z_real)\n",
    "                fake_scores = discriminator(z_fake.detach())\n",
    "                disc_loss = -torch.mean(real_scores) + torch.mean(fake_scores)\n",
    "                disc_loss.backward()\n",
    "                optimizer_disc.step()\n",
    "                for p in discriminator.parameters():\n",
    "                    p.data.clamp_(-CLIP_VALUE, CLIP_VALUE)\n",
    "\n",
    "        # 3) Generator\n",
    "        if batch_idx % N_CRITIC == 0:\n",
    "            optimizer_gen.zero_grad()\n",
    "            noise = torch.randn(seq.size(0), LATENT_DIM, device=DEVICE)\n",
    "            z_fake = generator(noise)\n",
    "            gen_loss = -torch.mean(discriminator(z_fake))\n",
    "            gen_loss.backward()\n",
    "            optimizer_gen.step()\n",
    "            epoch_gen += gen_loss.item()\n",
    "\n",
    "        epoch_recon += recon_loss.item()\n",
    "        epoch_contrast += contrast_loss.item()\n",
    "        epoch_encadv += enc_adv.item()\n",
    "        if batch_idx % N_CRITIC == 0:\n",
    "            epoch_disc += disc_loss.item()\n",
    "\n",
    "    n_batches = len(train_loader)\n",
    "    n_disc_updates = max(n_batches // N_CRITIC, 1)\n",
    "\n",
    "    avg_recon = epoch_recon / n_batches\n",
    "    avg_contrast = epoch_contrast / n_batches\n",
    "    avg_encadv = epoch_encadv / n_batches\n",
    "    avg_disc = epoch_disc / n_disc_updates\n",
    "    avg_gen = epoch_gen / n_disc_updates\n",
    "\n",
    "    history[\"epoch\"].append(epoch + 1)\n",
    "    history[\"recon_loss\"].append(avg_recon)\n",
    "    history[\"contrast_loss\"].append(avg_contrast)\n",
    "    history[\"enc_adv_loss\"].append(avg_encadv)\n",
    "    history[\"disc_loss\"].append(avg_disc)\n",
    "    history[\"gen_loss\"].append(avg_gen)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d}/{EPOCHS} | \"\n",
    "        f\"Recon: {avg_recon:.5f} | \"\n",
    "        f\"Contrast: {avg_contrast:.5f} | \"\n",
    "        f\"EncAdv: {avg_encadv:.5f} | \"\n",
    "        f\"Disc: {avg_disc:.5f} | \"\n",
    "        f\"Gen: {avg_gen:.5f}\"\n",
    "    )\n",
    "\n",
    "print(\"TRAINING COMPLETED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# TRAINING CURVES\n",
    "# ============================================================================\n",
    "\n",
    "if len(history[\"epoch\"]) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(\"Training Progress\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    axes[0, 0].plot(history[\"epoch\"], history[\"recon_loss\"], marker=\"o\")\n",
    "    axes[0, 0].set_title(\"Reconstruction Loss\")\n",
    "\n",
    "    axes[0, 1].plot(history[\"epoch\"], history[\"contrast_loss\"], marker=\"o\", color=\"orange\")\n",
    "    axes[0, 1].set_title(\"Contrastive Loss\")\n",
    "\n",
    "    axes[1, 0].plot(history[\"epoch\"], history[\"disc_loss\"], marker=\"s\", label=\"Disc\", color=\"red\")\n",
    "    axes[1, 0].plot(history[\"epoch\"], history[\"gen_loss\"], marker=\"^\", label=\"Gen\", color=\"green\")\n",
    "    axes[1, 0].set_title(\"GAN Losses\")\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "    axes[1, 1].plot(history[\"epoch\"], history[\"enc_adv_loss\"], marker=\"D\", color=\"purple\")\n",
    "    axes[1, 1].set_title(\"Encoder Adversarial Loss\")\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No history to plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2702ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# EVALUATION ON TEST SET\n",
    "# ============================================================================\n",
    "\n",
    "model.eval()\n",
    "all_scores = []\n",
    "all_labels = []\n",
    "\n",
    "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq, labels in test_loader:\n",
    "        seq = seq.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        z, recon = model(seq)\n",
    "        mse = F.mse_loss(recon, seq, reduction=\"none\")\n",
    "        mse = mse.mean(dim=(1, 2))\n",
    "        all_scores.append(mse.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "all_scores = np.concatenate(all_scores)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "roc_auc = roc_auc_score(all_labels, all_scores)\n",
    "precision, recall, _ = precision_recall_curve(all_labels, all_scores)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(all_labels, all_scores)\n",
    "youden = tpr - fpr\n",
    "best_idx = np.argmax(youden)\n",
    "best_thresh = thresholds[best_idx]\n",
    "\n",
    "print(\"ROC-AUC:\", roc_auc)\n",
    "print(\"PR-AUC: \", pr_auc)\n",
    "print(\"Best threshold:\", best_thresh)\n",
    "\n",
    "y_pred = (all_scores >= best_thresh).astype(int)\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(all_labels, y_pred, target_names=[\"Normal\", \"Anomaly\"]))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(all_labels, y_pred))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
